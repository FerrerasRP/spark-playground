{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:42:50.340466Z",
     "start_time": "2020-08-21T06:42:45.158683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.49:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>heart_disease</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff82b4bdfd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"heart_disease\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:42:56.586874Z",
     "start_time": "2020-08-21T06:42:50.347305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/datasets_heart.csv\", inferSchema=True, header=True)\n",
    "df.show(2)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:42:59.442882Z",
     "start_time": "2020-08-21T06:42:56.593739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1|  165|\n",
      "|     0|  138|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:42:59.512265Z",
     "start_time": "2020-08-21T06:42:59.445706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:43:01.867660Z",
     "start_time": "2020-08-21T06:42:59.519678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is clean with no null values\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(df)\n",
    "if null_columns_calc_list : \n",
    "    spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()\n",
    "else :\n",
    "    print(\"Data is clean with no null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:43:02.002838Z",
     "start_time": "2020-08-21T06:43:01.870487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in dependencies\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
    "from pyspark.sql.types import * \n",
    "\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:43:02.097245Z",
     "start_time": "2020-08-21T06:43:02.012761Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Data Prep function\n",
    "def MLClassifierDFPrep(df,input_columns,dependent_var,treat_outliers=True,treat_neg_values=True):\n",
    "    \n",
    "    # change label (class variable) to string type to prep for reindexing\n",
    "    # Pyspark is expecting a zero indexed integer for the label column. \n",
    "    # Just incase our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "    renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "    indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \n",
    "    indexed = indexer.fit(renamed).transform(renamed)\n",
    "    print(indexed.groupBy(dependent_var,\"label\").count().show(100))\n",
    "\n",
    "    # Convert all string type data in the input column list to numeric\n",
    "    # Otherwise the Algorithm will not be able to process it\n",
    "    numeric_inputs = []\n",
    "    string_inputs = []\n",
    "    for column in input_columns:\n",
    "        if str(indexed.schema[column].dataType) == 'StringType':\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "            indexed = indexer.fit(indexed).transform(indexed)\n",
    "            new_col_name = column+\"_num\"\n",
    "            string_inputs.append(new_col_name)\n",
    "        else:\n",
    "            numeric_inputs.append(column)\n",
    "            \n",
    "    if treat_outliers == True:\n",
    "        print(\"We are correcting for non normality now!\")\n",
    "        # empty dictionary d\n",
    "        d = {}\n",
    "        # Create a dictionary of quantiles\n",
    "        for col in numeric_inputs: \n",
    "            d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "        #Now fill in the values\n",
    "        for col in numeric_inputs:\n",
    "            skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "            skew = skew[0][0]\n",
    "            # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "            if skew > 1:\n",
    "                indexed = indexed.withColumn(col, \\\n",
    "                log(when(df[col] < d[col][0],d[col][0])\\\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(indexed[col] ) +1).alias(col))\n",
    "                print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "            elif skew < -1:\n",
    "                indexed = indexed.withColumn(col, \\\n",
    "                exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(indexed[col] )).alias(col))\n",
    "                print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
    "\n",
    "            \n",
    "    # Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
    "    # Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
    "    minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) # Calculate the mins for all columns in the df\n",
    "    min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) # Create an array for all mins and select only the input cols\n",
    "    df_minimum = min_array.select(array_min(min_array.mins)).collect() # Collect golobal min as Python object\n",
    "    df_minimum = df_minimum[0][0] # Slice to get the number itself\n",
    "\n",
    "    features_list = numeric_inputs + string_inputs\n",
    "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "    output = assembler.transform(indexed).select('features','label')\n",
    "\n",
    "#     final_data = output.select('features','label') #drop everything else\n",
    "    \n",
    "    # Now check for negative values and ask user if they want to correct that? \n",
    "    if df_minimum < 0:\n",
    "        print(\" \")\n",
    "        print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
    "        print(\" \")\n",
    "    \n",
    "    if treat_neg_values == True:\n",
    "        print(\"You have opted to correct that by rescaling all your features to a range of 0 to 1\")\n",
    "        print(\" \")\n",
    "        print(\"We are rescaling you dataframe....\")\n",
    "        scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "        # Compute summary statistics and generate MinMaxScalerModel\n",
    "        scalerModel = scaler.fit(output)\n",
    "\n",
    "        # rescale each feature to range [min, max].\n",
    "        scaled_data = scalerModel.transform(output)\n",
    "        final_data = scaled_data.select('label','scaledFeatures') # added class to the selection\n",
    "        final_data = final_data.withColumnRenamed('scaledFeatures','features')\n",
    "        print(\"Done!\")\n",
    "\n",
    "    else:\n",
    "        print(\"You have opted not to correct that therefore you will not be able to use to Naive Bayes classifier\")\n",
    "        print(\"We will return the dataframe unscaled.\")\n",
    "        final_data = output\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:43:02.258118Z",
     "start_time": "2020-08-21T06:43:02.109540Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ClassTrainEval(classifier,features,classes,folds,train,test):\n",
    "    \n",
    "    def FindMtype(classifier):\n",
    "        # Intstantiate Model\n",
    "        M = classifier\n",
    "        # Learn what it is\n",
    "        Mtype = type(M).__name__\n",
    "        \n",
    "        return Mtype\n",
    "    \n",
    "    Mtype = FindMtype(classifier)\n",
    "    \n",
    "\n",
    "    def IntanceFitModel(Mtype,classifier,classes,features,folds,train):\n",
    "        \n",
    "        if Mtype == \"OneVsRest\":\n",
    "            # instantiate the base classifier.\n",
    "            lr = LogisticRegression()\n",
    "            # instantiate the One Vs Rest Classifier.\n",
    "            OVRclassifier = OneVsRest(classifier=lr)\n",
    "#             fitModel = OVRclassifier.fit(train)\n",
    "            # Add parameters of your choice here:\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "            #Cross Validator requires the following parameters:\n",
    "            crossval = CrossValidator(estimator=OVRclassifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=folds) # 3 is best practice\n",
    "            # Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            # specify layers for the neural network:\n",
    "            # input layer of size features, two intermediate of features+1 and same size as features\n",
    "            # and output of size number of classes\n",
    "            # Note: crossvalidator cannot be used here\n",
    "            features_count = len(features[0][0])\n",
    "            layers = [features_count, features_count+1, features_count, classes]\n",
    "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "            fitModel = MPC_classifier.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2: # These classifiers currently only accept binary classification\n",
    "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
    "            return\n",
    "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
    "  \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LogisticRegression\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,20])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"NaiveBayes\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LinearSVC\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .build())\n",
    "            \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .build())\n",
    "            \n",
    "            #Cross Validator requires all of the following parameters:\n",
    "            crossval = CrossValidator(estimator=classifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=folds) # 3 + is best practice\n",
    "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "    \n",
    "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,folds,train)\n",
    "    \n",
    "    # Print feature selection metrics\n",
    "    if fitModel is not None:\n",
    "        \n",
    "        if Mtype in(\"OneVsRest\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            global OVR_BestModel\n",
    "            OVR_BestModel = BestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            # Extract list of binary models\n",
    "            models = BestModel.models\n",
    "            for model in models:\n",
    "                print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept)\n",
    "                print('\\033[1m' + 'Top 20 Coefficients:'+ '\\033[0m')\n",
    "                coeff_array = model.coefficients.toArray()\n",
    "                coeff_scores = []\n",
    "                for x in coeff_array:\n",
    "                    coeff_scores.append(float(x))\n",
    "                # Then zip with input_columns list and create a df\n",
    "                result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "                print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "\n",
    "\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            print(\"\")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
    "            print(\"\")\n",
    "            global MLPC_Model\n",
    "            MLPC_BestModel = fitModel\n",
    "\n",
    "        if Mtype in(\"DecisionTreeClassifier\", \"GBTClassifier\",\"RandomForestClassifier\"):\n",
    "            # FEATURE IMPORTANCES\n",
    "            # Estimate of the importance of each feature.\n",
    "            # Each feature’s importance is the average of its importance across all trees \n",
    "            # in the ensemble The importance vector is normalized to sum to 1. \n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Top 20 Feature Importances\"+ '\\033[0m')\n",
    "            print(\"(Scores add up to 1)\")\n",
    "            print(\"Lowest score is the least important\")\n",
    "            print(\" \")\n",
    "            featureImportances = BestModel.featureImportances.toArray()\n",
    "            # Convert from numpy array to list\n",
    "            imp_scores = []\n",
    "            for x in featureImportances:\n",
    "                imp_scores.append(float(x))\n",
    "            # Then zip with input_columns list and create a df\n",
    "            result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "            print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "            \n",
    "            # Save the feature importance values and the models\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                global DT_featureimportances\n",
    "                DT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global DT_BestModel\n",
    "                DT_BestModel = BestModel\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                global GBT_featureimportances\n",
    "                GBT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global GBT_BestModel\n",
    "                GBT_BestModel = BestModel\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                global RF_featureimportances\n",
    "                RF_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global RF_BestModel\n",
    "                RF_BestModel = BestModel\n",
    "\n",
    "        # Print the coefficients\n",
    "        if Mtype in(\"LogisticRegression\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "            print('\\033[1m' + \" Top 20 Coefficients\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "            # Convert from numpy array to list\n",
    "            coeff_array = BestModel.coefficientMatrix.toArray()\n",
    "            coeff_scores = []\n",
    "            for x in coeff_array[0]:\n",
    "                coeff_scores.append(float(x))\n",
    "            # Then zip with input_columns list and create a df\n",
    "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "            # Save the coefficient values and the models\n",
    "            global LR_coefficients\n",
    "            LR_coefficients = BestModel.coefficientMatrix.toArray()\n",
    "            global LR_BestModel\n",
    "            LR_BestModel = BestModel\n",
    "\n",
    "        # Print the Coefficients\n",
    "        if Mtype in(\"LinearSVC\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            print(\"Intercept: \" + str(BestModel.intercept))\n",
    "            print('\\033[1m' + \"Top 20 Coefficients\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "#             print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
    "            coeff_array = BestModel.coefficients.toArray()\n",
    "            coeff_scores = []\n",
    "            for x in coeff_array:\n",
    "                coeff_scores.append(float(x))\n",
    "            # Then zip with input_columns list and create a df\n",
    "            result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "            print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "            # Save the coefficient values and the models\n",
    "            global LSVC_coefficients\n",
    "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
    "            global LSVC_BestModel\n",
    "            LSVC_BestModel = BestModel\n",
    "        \n",
    "   \n",
    "    # Set the column names to match the external results dataframe that we will join with later:\n",
    "    columns = ['Classifier', 'Result']\n",
    "    \n",
    "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
    "        Mtype = [Mtype] # make this a list\n",
    "        score = [\"N/A\"]\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "    else:\n",
    "        predictions = fitModel.transform(test)\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        Mtype = [Mtype] # make this a string\n",
    "        score = [str(accuracy)] #make this a string and convert to a list\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        \n",
    "    return result\n",
    "    #Also returns the fit model important scores or p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:43:03.620375Z",
     "start_time": "2020-08-21T06:43:02.261729Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up independ and dependent vars\n",
    "input_columns = df.columns\n",
    "input_columns = input_columns[:-1] # keep only relevant columns: everything but the first and last cols\n",
    "dependent_var = 'target'\n",
    "\n",
    "# Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
    "class_count = df.select(countDistinct(\"target\")).collect()\n",
    "classes = class_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:46:37.434701Z",
     "start_time": "2020-08-21T06:46:37.419638Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SparkMl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4f2acd3ed376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../lib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkMl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SparkMl'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "from sparkml import SparkMl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T06:46:16.062000Z",
     "start_time": "2020-08-21T06:46:16.031616Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f9b62006bcab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msparkml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkml' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "sparkml = importlib.reload(sparkml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T06:34:32.104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|target|label|count|\n",
      "+------+-----+-----+\n",
      "|     1|  0.0|  165|\n",
      "|     0|  1.0|  138|\n",
      "+------+-----+-----+\n",
      "\n",
      "None\n",
      "We are correcting for non normality now!\n",
      "chol has been treated for positive (right) skewness. (skew =) 1.1377326187082237 )\n",
      "fbs has been treated for positive (right) skewness. (skew =) 1.9768034646834516 )\n",
      "oldpeak has been treated for positive (right) skewness. (skew =) 1.2634255245891595 )\n",
      "ca has been treated for positive (right) skewness. (skew =) 1.303925955673585 )\n",
      "You have opted to correct that by rescaling all your features to a range of 0 to 1\n",
      " \n",
      "We are rescaling you dataframe....\n",
      "Done!\n",
      " \n",
      "\u001b[1mLogisticRegression\u001b[0m\n",
      "Intercept: [-0.2946093807248298]\n",
      "\u001b[1m Top 20 Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "+--------+-------------------+\n",
      "|feature |coeff              |\n",
      "+--------+-------------------+\n",
      "|ca      |3.3556048188344705 |\n",
      "|trestbps|2.5710066348183185 |\n",
      "|chol    |1.8912913794411461 |\n",
      "|oldpeak |1.7821224941175056 |\n",
      "|sex     |1.5027823003094942 |\n",
      "|thal    |1.2726516296871815 |\n",
      "|exang   |1.0356416677880285 |\n",
      "|fbs     |0.19369197705759372|\n",
      "|age     |-0.9548393202733119|\n",
      "|restecg |-1.18774076141198  |\n",
      "|slope   |-1.209639602997717 |\n",
      "|cp      |-3.317128855443783 |\n",
      "|thalach |-4.2784953772553935|\n",
      "+--------+-------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m 3.877124712851719\n",
      "\u001b[1mTop 20 Coefficients:\u001b[0m\n",
      "+--------+--------------------+\n",
      "|feature |coeff               |\n",
      "+--------+--------------------+\n",
      "|cp      |2.8737173821987563  |\n",
      "|thalach |1.9770053839800474  |\n",
      "|slope   |0.6391014787723566  |\n",
      "|restecg |0.603251173760043   |\n",
      "|fbs     |-0.18922004201950932|\n",
      "|age     |-0.6206415103568728 |\n",
      "|exang   |-1.196027984854824  |\n",
      "|sex     |-1.458620770823419  |\n",
      "|thal    |-1.7057896409103535 |\n",
      "|oldpeak |-2.2801547028224274 |\n",
      "|trestbps|-2.3590795001408944 |\n",
      "|chol    |-2.5943372345239317 |\n",
      "|ca      |-2.7918992083361642 |\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      "\u001b[1mIntercept: \u001b[0m -3.8771247125957244\n",
      "\u001b[1mTop 20 Coefficients:\u001b[0m\n",
      "+--------+-------------------+\n",
      "|feature |coeff              |\n",
      "+--------+-------------------+\n",
      "|ca      |2.791899208467346  |\n",
      "|chol    |2.59433723417095   |\n",
      "|trestbps|2.359079500081923  |\n",
      "|oldpeak |2.2801547028457563 |\n",
      "|thal    |1.7057896407693007 |\n",
      "|sex     |1.4586207708018293 |\n",
      "|exang   |1.1960279849122806 |\n",
      "|age     |0.620641510454211  |\n",
      "|fbs     |0.18922004201689696|\n",
      "|restecg |-0.6032511737309472|\n",
      "|slope   |-0.6391014788643172|\n",
      "|thalach |-1.9770053839244996|\n",
      "|cp      |-2.8737173821756556|\n",
      "+--------+-------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mLinearSVC\u001b[0m\n",
      "Intercept: -0.0897664148056383\n",
      "\u001b[1mTop 20 Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "+--------+-------------------+\n",
      "|feature |coeff              |\n",
      "+--------+-------------------+\n",
      "|ca      |1.2452002199259689 |\n",
      "|thal    |0.7825208724242344 |\n",
      "|oldpeak |0.7586337524043308 |\n",
      "|trestbps|0.628997744808587  |\n",
      "|exang   |0.5413042181281297 |\n",
      "|sex     |0.41224088358323485|\n",
      "|chol    |0.30543557816045835|\n",
      "|fbs     |0.03882779891760559|\n",
      "|age     |-0.0721678504608175|\n",
      "|restecg |-0.3401389592394804|\n",
      "|slope   |-0.4862272034748238|\n",
      "|cp      |-1.5882545676745061|\n",
      "|thalach |-1.8180068131302058|\n",
      "+--------+-------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Top 20 Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------+---------------------+\n",
      "|feature |score                |\n",
      "+--------+---------------------+\n",
      "|cp      |0.2074782175386285   |\n",
      "|ca      |0.16240724655415803  |\n",
      "|oldpeak |0.1244808334485584   |\n",
      "|thalach |0.10632586799454162  |\n",
      "|thal    |0.08049807886946908  |\n",
      "|exang   |0.06616071404745179  |\n",
      "|age     |0.058901935684934695 |\n",
      "|chol    |0.05721372567086423  |\n",
      "|slope   |0.04903736719381064  |\n",
      "|trestbps|0.043250976814225336 |\n",
      "|sex     |0.022136824382998223 |\n",
      "|restecg |0.019031315975510395 |\n",
      "|fbs     |0.0030768958248490715|\n",
      "+--------+---------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Call on data prep, train and evaluate functions\n",
    "test1_data = SparkML.mlClassifierDfPrep(df,input_columns,dependent_var)\n",
    "test1_data.limit(5).toPandas()\n",
    "\n",
    "# Comment out Naive Bayes if your data still contains negative values\n",
    "classifiers = [\n",
    "                LogisticRegression()\n",
    "                ,OneVsRest()\n",
    "               ,LinearSVC()\n",
    "               ,NaiveBayes()\n",
    "               ,RandomForestClassifier()\n",
    "               ,GBTClassifier()\n",
    "               ,DecisionTreeClassifier()\n",
    "               ,MultilayerPerceptronClassifier()\n",
    "              ] \n",
    "\n",
    "train,test = test1_data.randomSplit([0.8,0.2])\n",
    "features = test1_data.select(['features']).collect()\n",
    "folds = 3 # because we have limited data\n",
    "\n",
    "#set up your results table\n",
    "columns = ['Classifier', 'Result']\n",
    "vals = [(\"Place Holder\",\"N/A\")]\n",
    "results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    new_result = SparkML.classTrainEval(spark,input_columns,classifier,features,classes,folds,train,test)\n",
    "    results = results.union(new_result)\n",
    "results = results.where(\"Classifier!='Place Holder'\")\n",
    "print(\"!!!!!Final Results!!!!!!!!\")\n",
    "results.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-21T06:34:39.710Z"
    }
   },
   "outputs": [],
   "source": [
    "test.show(2)\n",
    "predictions = SparkML.LR_BestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:42:46.533321Z",
     "start_time": "2020-08-20T09:42:46.349448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|[0.22916666666666...|[9.93240557782315...|[0.49662027889115...|       1.0|\n",
      "|  0.0|[0.47916666666666...|[8.28858684899354...|[0.41442934244967...|       1.0|\n",
      "|  0.0|[0.5,1.0,0.0,0.45...|[6.95452480639844...|[0.34772624031992...|       1.0|\n",
      "|  0.0|[0.60416666666666...|[8.63928903334237...|[0.43196445166711...|       1.0|\n",
      "|  0.0|[0.72916666666666...|[4.18041635940408...|[0.20902081797020...|       1.0|\n",
      "|  1.0|(13,[0,3,4,7,9,11...|[6.59084642934599...|[0.32954232146729...|       1.0|\n",
      "|  1.0|[0.125,1.0,0.0,0....|[8.83906131390991...|[0.44195306569549...|       1.0|\n",
      "|  1.0|[0.35416666666666...|[9.83123131422019...|[0.49156156571100...|       1.0|\n",
      "|  1.0|[0.41666666666666...|[7.35969538785277...|[0.36798476939263...|       1.0|\n",
      "|  1.0|[0.45833333333333...|[8.42678061215552...|[0.42133903060777...|       1.0|\n",
      "|  1.0|[0.47916666666666...|[9.79244188802178...|[0.48962209440108...|       1.0|\n",
      "|  1.0|[0.52083333333333...|[2.58679847631251...|[0.12933992381562...|       1.0|\n",
      "|  1.0|[0.54166666666666...|[4.78012430211043...|[0.23900621510552...|       1.0|\n",
      "|  1.0|[0.54166666666666...|[3.23614912566316...|[0.16180745628315...|       1.0|\n",
      "|  1.0|[0.5625,1.0,0.0,0...|[5.13803570416750...|[0.25690178520837...|       1.0|\n",
      "|  1.0|[0.58333333333333...|[5.46906154463072...|[0.27345307723153...|       1.0|\n",
      "|  1.0|[0.58333333333333...|[2.81710150661554...|[0.14085507533077...|       1.0|\n",
      "|  1.0|[0.60416666666666...|[4.82913486784731...|[0.24145674339236...|       1.0|\n",
      "|  1.0|[0.60416666666666...|[9.09942721138029...|[0.45497136056901...|       1.0|\n",
      "|  1.0|[0.60416666666666...|[6.14017720609612...|[0.30700886030480...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(\"prediction==1\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:51:08.461492Z",
     "start_time": "2020-08-20T09:51:07.143008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   41|\n",
      "|  1.0|   25|\n",
      "+-----+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|   42|\n",
      "|       1.0|   24|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.groupBy(\"label\").count().show()\n",
    "predictions.groupBy(\"prediction\").count().show()\n",
    "\n",
    "predictions.filter(\"prediction != label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T09:38:29.042248Z",
     "start_time": "2020-08-20T09:38:22.626362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1mLogisticRegression\u001b[0m\n",
      "Intercept: [-0.5428673708155662]\n",
      "\u001b[1m Top 20 Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "+--------+--------------------+\n",
      "|feature |coeff               |\n",
      "+--------+--------------------+\n",
      "|ca      |3.1841471866896907  |\n",
      "|trestbps|3.0785637047996857  |\n",
      "|chol    |2.3174290983596824  |\n",
      "|thal    |2.0766233787636557  |\n",
      "|sex     |1.7292613165951467  |\n",
      "|exang   |0.8672991707579711  |\n",
      "|oldpeak |0.7577523093299948  |\n",
      "|fbs     |-0.18247353508713882|\n",
      "|age     |-0.7301022598870994 |\n",
      "|restecg |-0.8417228068245027 |\n",
      "|slope   |-2.185381652517893  |\n",
      "|cp      |-2.671354499251751  |\n",
      "|thalach |-4.456346558009576  |\n",
      "+--------+--------------------+\n",
      "\n",
      "None\n",
      "+------------------+------+\n",
      "|Classifier        |Result|\n",
      "+------------------+------+\n",
      "|LogisticRegression|87.95 |\n",
      "+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "classifiers = [ LogisticRegression()\n",
    "                ] \n",
    "\n",
    "#Select the top n features and view results\n",
    "n = 99\n",
    "\n",
    "# For Logistic regression or One vs Rest\n",
    "selector = ChiSqSelector(numTopFeatures=n, featuresCol=\"features\",\n",
    "                     outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "bestFeaturesDf = selector.fit(test1_data).transform(test1_data)\n",
    "bestFeaturesDf = bestFeaturesDf.select(\"label\",\"selectedFeatures\")\n",
    "bestFeaturesDf = bestFeaturesDf.withColumnRenamed(\"selectedFeatures\",\"features\")\n",
    "\n",
    "# Collect features\n",
    "features = bestFeaturesDf.select(['features']).collect()\n",
    "\n",
    "# Split\n",
    "train,test = bestFeaturesDf.randomSplit([0.7,0.3])\n",
    "\n",
    "# Specify folds\n",
    "folds = 3\n",
    "\n",
    "#set up your results table\n",
    "columns = ['Classifier', 'Result']\n",
    "vals = [(\"Place Holder\",\"N/A\")]\n",
    "results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    new_result = ClassTrainEval(classifier,features,classes,folds,train,test)\n",
    "    results = results.union(new_result)\n",
    "results = results.where(\"Classifier!='Place Holder'\")\n",
    "results.show(100,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
