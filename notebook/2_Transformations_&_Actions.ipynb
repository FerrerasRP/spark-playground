{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark RDD Operations\n",
    "\n",
    "Apache Spark RDD supports two types of Operations-\n",
    "\n",
    "- Transformations\n",
    "- Actions\n",
    "\n",
    "## RDD Transformation\n",
    "\n",
    "Spark Transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output. Each time it creates new RDD when we apply any transformation. Thus, the so input RDDs, cannot be changed since RDD are immutable in nature.\n",
    "\n",
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronavirus', 'disease', '2019', '(covid-19)', 'is', 'an', 'infectious', 'disease', 'caused', 'by', 'severe', 'acute', 'respiratory', 'syndrome', 'coronavirus', '2', '(sars-cov-2).[10]', 'it', 'was', 'first', 'identified', 'in', 'december', '2019', 'in', 'wuhan,', 'china,', 'and', 'has', 'resulted', 'in', 'an', 'ongoing', 'pandemic.[11][12]', 'the', 'first', 'confirmed', 'case', 'has', 'been', 'traced', 'back', 'to', '17', 'november', '2019.[13]', 'traces', 'of', 'the', 'virus', 'have', 'been', 'found', 'in', 'wastewater', 'that', 'was', 'collected', 'from', 'milan', 'and', 'turin,', 'italy', 'on', '18', 'december', '2019.[14]', 'as', 'of', '25', 'june', '2020,', 'more', 'than', '9.39', 'million', 'cases', 'have', 'been', 'reported', 'across', '188', 'countries', 'and', 'territories,', 'resulting', 'in', 'more', 'than', '481,000', 'deaths.', 'more', 'than', '4.71', 'million', 'people', 'have', 'recovered.[9]']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"music\").getOrCreate()\n",
    "txtData = spark.read.text('../data/covid19.txt')\n",
    "data = txtData.rdd.map(lambda x: x.value.lower().split()).collect()\n",
    "for d in data[:1]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus\n",
      "disease\n",
      "2019\n",
      "(COVID-19)\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "data = txtData.rdd.flatMap(lambda line : line.value.split(\" \")).collect()\n",
    "for d in data[:5]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Filter(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus\n",
      "disease\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "data = txtData.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    ".filter(lambda x: x!='').collect()\n",
    "for d in data[:3]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coronaviru', <pyspark.resultiterable.ResultIterable object at 0x7f2feb2b2240>)\n",
      "('disease', <pyspark.resultiterable.ResultIterable object at 0x7f2feb2b25f8>)\n",
      "('2019', <pyspark.resultiterable.ResultIterable object at 0x7f2feb2b2748>)\n"
     ]
    }
   ],
   "source": [
    "data = txtData.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    ".filter(lambda x: x!='').groupBy(lambda w: w[0:10]).collect()\n",
    "for d in data[:3]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByey / ReduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = txtData.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    ".filter(lambda x: x!='')\n",
    "rdd3_mapped = data.map(lambda x: (x,1))\n",
    "rdd3_mapped\n",
    "rdd3_grouped = rdd3_mapped.groupByKey().collect()[:3]\n",
    "rdd3_grouped\n",
    "rdd3_grouped = rdd3_mapped.reduceByKey(lambda a,b : a+b ).collect()[:3]\n",
    "rdd3_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59bff5042475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"jan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"nov\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2014\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"feb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2014\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrdd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2014\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrdd3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2011\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"may\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrdd4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrdd4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(1,\"jan\",2016),(3,\"nov\",2014),(16,\"feb\",2014)])\n",
    "rdd2 = spark.createDataFrame([(5,\"dec\",2014),(17,\"sep\",2015)])\n",
    "rdd3 = spark.createDataFrame([(6,\"dec\",2011),(16,\"may\",2015)])\n",
    "rdd4 = rdd1.union(rdd2).union(rdd3)\n",
    "rdd4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _2|\n",
      "+---+\n",
      "|dec|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(6,\"jan\",2016),(3,\"nov\",2014),(16,\"feb\",2014)])\n",
    "rdd2 = spark.createDataFrame([(5,\"dec\",2014),(16,\"sep\",2015)])\n",
    "rdd3 = spark.createDataFrame([(6,\"dec\",2011),(16,\"may\",2015)])\n",
    "rdd4 = rdd3.select(\"_2\").intersect(rdd2.select(\"_2\"))\n",
    "rdd4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _2|\n",
      "+---+\n",
      "|sep|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(5,\"dec\",2014),(16,\"sep\",2015)])\n",
    "rdd2 = spark.createDataFrame([(6,\"dec\",2011),(16,\"may\",2015)])\n",
    "rdd3 = rdd1.select(\"_2\").subtract(rdd2.select(\"_2\"))\n",
    "rdd3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(_2='dec'), Row(_2='dec')),\n",
       " (Row(_2='dec'), Row(_2='may')),\n",
       " (Row(_2='sep'), Row(_2='dec')),\n",
       " (Row(_2='sep'), Row(_2='may'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(5,\"dec\",2014),(16,\"sep\",2015)])\n",
    "rdd2 = spark.createDataFrame([(6,\"dec\",2011),(16,\"may\",2015)])\n",
    "rdd3 = rdd1.select(\"_2\").rdd.cartesian(rdd2.select(\"_2\").rdd)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  6|\n",
      "|  5|\n",
      "|  3|\n",
      "| 16|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(6,\"jan\",2016),(3,\"nov\",2014),(16,\"feb\",2014),(5,\"dec\",2014),(16,\"sep\",2015)])\n",
    "rdd2 = rdd1.select(\"_1\").distinct()\n",
    "rdd2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=3, _2='nov', _3=2014),\n",
       " Row(_1=16, _2='feb', _3=2014),\n",
       " Row(_1=5, _2='dec', _3=2014),\n",
       " Row(_1=16, _2='sep', _3=2015),\n",
       " Row(_1=6, _2='jan', _3=2016)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(6,\"jan\",2016),(3,\"nov\",2014),(16,\"feb\",2014),(5,\"dec\",2014),(16,\"sep\",2015)])\n",
    "rdd2 = rdd1.rdd.sortBy(lambda x: x[2]).collect()\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Action\n",
    "\n",
    "Transformations create RDDs from each other, but when we want to work with the actual dataset, at that point action is performed. When the action is triggered after the result, new RDD is not formed like transformation. Thus, Actions are Spark RDD operations that give non-RDD values. The values of action are stored to drivers or to the external storage system. It brings laziness of RDD into motion.\n",
    "\n",
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = txtData.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    ".filter(lambda x: x!='')\n",
    "counts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus', 'disease', '2019']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = txtData.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    ".filter(lambda x: x!='').collect()[:3]\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus', 'disease', '2019']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = txtData.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    ".filter(lambda x: x!='')\n",
    "counts.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who', 'where', 'wastewater']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = txtData.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    ".filter(lambda x: x!='')\n",
    "counts.top(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {Row(_1=6): 1, Row(_1=3): 1, Row(_1=16): 2, Row(_1=5): 1})"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(6,\"jan\",2016),(3,\"nov\",2014),(16,\"feb\",2014),(5,\"dec\",2014),(16,\"sep\",2015)])\n",
    "rdd2 = rdd1.select(\"_1\").rdd.countByValue()\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|sum(_1)|\n",
      "+-------+\n",
      "|     46|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.createDataFrame([(6,\"jan\",2016),(3,\"nov\",2014),(16,\"feb\",2014),(5,\"dec\",2014),(16,\"sep\",2015)])\n",
    "rdd2 = rdd1.select(\"_1\")\n",
    "rdd2.groupBy().sum().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
