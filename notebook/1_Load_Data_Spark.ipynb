{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data to Spark\n",
    "\n",
    "PySpark can create distributed datasets from any storage source supported by Hadoop, including the local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- SignDate: string (nullable = true)\n",
      " |-- Status: integer (nullable = true)\n",
      " |-- Level: integer (nullable = true)\n",
      " |-- Campaign: integer (nullable = true)\n",
      " |-- LinkedWithApps: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"music\").getOrCreate()\n",
    "cust = spark.read.csv('../data/cust.csv',inferSchema=True, header= True)\n",
    "cust.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.json('../data/data.json')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.load(\"../data/users.parquet\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from AWS s3\n",
    "\n",
    "Pyspark script for downloading a single parquet file from Amazon S3 via the s3a protocol. It also reads the credentials from the \"~/.aws/credentials\", so we don't need to hardcode them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Some constants\n",
    "#\n",
    "aws_profile = \"your_profile\"\n",
    "aws_region = \"your_region\"\n",
    "s3_bucket = \"your_bucket\"\n",
    "\n",
    "# \n",
    "# Reading environment variables from aws credential file \n",
    "#\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\") \n",
    "\n",
    "# \n",
    "# Configuring pyspark\n",
    "#\n",
    "\n",
    "# see https://github.com/jupyter/docker-stacks/issues/127#issuecomment-214594895 \n",
    "# and https://github.com/radanalyticsio/pyspark-s3-notebook/blob/master/s3-source-example.ipynb\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\"\n",
    "\n",
    "# If this doesn't work you might have to delete your ~/.ivy2 directory to reset your package cache.\n",
    "# (see https://github.com/databricks/spark-redshift/issues/244#issuecomment-239950148)\n",
    "import pyspark\n",
    "sc=pyspark.SparkContext()\n",
    "# see https://github.com/databricks/spark-redshift/issues/298#issuecomment-271834485\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "\n",
    "# see https://stackoverflow.com/questions/28844631/how-to-set-hadoop-configuration-values-from-pyspark\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "# see https://stackoverflow.com/questions/43454117/how-do-you-use-s3a-with-spark-2-1-0-on-aws-us-east-2\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", access_id)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", access_key)\n",
    "\n",
    "# see http://blog.encomiabile.it/2015/10/29/apache-spark-amazon-s3-and-apache-mesos/\n",
    "hadoop_conf.set(\"fs.s3a.connection.maximum\", \"100000\")\n",
    "\n",
    "# see https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")\n",
    "\n",
    "#\n",
    "# Downloading the parquet file\n",
    "#\n",
    "sql=pyspark.sql.SparkSession(sc)\n",
    "path = s3_bucket + \"your_path\"\n",
    "dataS3=sql.read.parquet(\"s3a://\" + path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
